{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python36964bit455cf936f5d74eed9470ab769c9b3f8c",
   "display_name": "Python 3.6.9 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "### Loading the models trained on Ejecta DVR images and saving the predicted images"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from PIL import ImageDraw\n",
    "from model import models\n",
    "from model import metrics\n",
    "from data import camera_utils\n",
    "from data import data_utils\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global params\n",
    "\n",
    "width = 512\n",
    "height = 512\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# volume params\n",
    "center = torch.Tensor([0.0, 0.0, 0.0])\n",
    "radius = 1.0\n",
    "vol_params = (\"sphere\", center, radius)\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "# load saved model\n",
    "model_name = 'NeRF'\n",
    "input_ch = 3\n",
    "points_type = 'cartesian'\n",
    "viewdir_type = 'cartesian'\n",
    "out_dir = '/home/goel/Thesis/Code/dvr/outputs/nerf_approach/first_run/lr1e-2/points_128/'\n",
    "writer_path = os.path.join(out_dir,'logs')\n",
    "model_path = os.path.join(out_dir,'models/200.pth')\n",
    "\n",
    "# create the model\n",
    "model = models.OldNeRF(input_ch=input_ch, output_ch = 4, hidden_size=128).to(device)\n",
    "\n",
    "checkpoint = torch.load(model_path, map_location=device)\n",
    "input_map = checkpoint['input_map']\n",
    "model.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_model_size(model):\n",
    "    num_params = 0\n",
    "    traininable_param = 0\n",
    "    for param in model.parameters():\n",
    "        num_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            traininable_param += param.numel()\n",
    "    print(\"[Network  Total number of parameters : %.3f M\" % (num_params / 1e6))\n",
    "    print(\n",
    "        \"[Network  Total number of trainable parameters : %.3f M\"\n",
    "        % (traininable_param / 1e6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_data_dir = '/home/goel/Thesis/Data/ejecta_27/train/'\n",
    "# train_data_lst = []\n",
    "\n",
    "# for cam_file in glob.glob(train_data_dir+\"*.json\"):\n",
    "#     fid =cam_file[cam_file.rfind('_')+1:cam_file.rfind('.')]\n",
    "#     train_data_lst.append(fid)\n",
    "\n",
    "# n_data = len(train_data_lst)\n",
    "# train_selected_data = random.sample(range(0,n_data),4)\n",
    "\n",
    "# gt_train_data = []\n",
    "# pred_train_data = []\n",
    "\n",
    "# for data_idx in train_selected_data:\n",
    "#     id = train_data_lst[data_idx]\n",
    "#     cam_file = train_data_dir + \"camera_\" + id + \".json\"\n",
    "#     img_file = train_data_dir + \"color_\" + id + \".png\"\n",
    "\n",
    "#     points, viewdirs, valid_mask = data_utils.get_input_data(cam_file, vol_params, points_type, viewdir_type)\n",
    "\n",
    "#     input = torch.cat((points, viewdirs),dim=0).to(device)\n",
    "#     input = data_utils.input_mapping(input, input_map, True, True, \"cartesian\")\n",
    "\n",
    "#     input = input.unsqueeze_(0).to(device)\n",
    "#     rgb = model(input)\n",
    "#     clamped_rgb = torch.clamp(rgb.detach(), min=0.0, max=1.0)\n",
    "#     pred_train_data.append(clamped_rgb[0])\n",
    "\n",
    "#     img = torch.Tensor(np.array(Image.open(img_file)) / 255.).permute(2,0,1)[:3]\n",
    "#     gt_train_data.append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gt_train = torch.stack(gt_train_data,dim=0)\n",
    "# pred_train = torch.stack(pred_train_data,dim=0)\n",
    "# gt_val = torch.stack(gt_val_data,dim=0)\n",
    "# pred_val = torch.stack(pred_val_data,dim=0)\n",
    "\n",
    "# writer = SummaryWriter(writer_path)\n",
    "# # writer.add_images('vis_gt_train', gt_train, 100)\n",
    "# # writer.add_images('vis_pred_train', pred_train, 100)\n",
    "# writer.add_images('vis_gt_val', gt_val, 101)\n",
    "# writer.add_images('vis_pred_val', pred_val, 101)"
   ]
  },
  {
   "source": [
    "### Creating zoomed in views of ground truth and network predictions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Image Size:  (400, 400)\nSize of the crop taken:  (70, 70)\nSize of the scaled crop:  (210, 210)\nPosition where the crop is taken:  (180, 90, 250, 160)\nPosition where scaled crop is pasted (0, 190, 210, 400)\n"
     ]
    }
   ],
   "source": [
    "img_file = '/home/parika/Downloads/ConvNet/gt.png'\n",
    "created_img = '/home/parika/Downloads/ConvNet/gt_with_zoom.png'\n",
    "gt_img_orig = Image.open(img_file)\n",
    "gt_img_orig.load()\n",
    "\n",
    "gt_img_white_bkg = Image.new(\"RGB\", gt_img_orig.size, (0, 0, 0))\n",
    "gt_img_white_bkg.paste(gt_img_orig, mask=gt_img_orig.split()[3])\n",
    "\n",
    "borders = (80,80,480,480)\n",
    "gt_img = gt_img_white_bkg.crop(borders)\n",
    "print(\"Image Size: \", gt_img.size)\n",
    "\n",
    "# borders_crop = (210,210,280,280)\n",
    "# borders_crop = (220,70,290,140)\n",
    "# borders_crop = (170,70,240,140)\n",
    "borders_crop = (180, 90, 250, 160)\n",
    "gt_crop = gt_img.crop(borders_crop)\n",
    "print(\"Size of the crop taken: \", gt_crop.size)\n",
    "\n",
    "height, width = gt_crop.size\n",
    "rescaled_size=(height*3, width*3)\n",
    "gt_crop_scaled =  gt_crop.resize(rescaled_size)\n",
    "print(\"Size of the scaled crop: \",gt_crop_scaled.size)\n",
    "\n",
    "# Draw a box to specify where the crop is taken\n",
    "width_of_rect = 1.0\n",
    "draw = ImageDraw.Draw(gt_img)\n",
    "draw.rectangle(borders_crop, outline='black', width=3)\n",
    "print(\"Position where the crop is taken: \",borders_crop)\n",
    "\n",
    "# Draw a box where the crop will be pasted\n",
    "# pos_paste_crop = (0,0,rescaled_size[0],rescaled_size[1])\n",
    "# draw.rectangle(pos_paste_crop, outline='black', width=3)\n",
    "# print(\"Position where scaled crop is pasted\",pos_paste_crop)\n",
    "\n",
    "# pos_paste_crop = (0,0,rescaled_size[0],rescaled_size[1])\n",
    "pos_paste_crop = (0,gt_img.size[1]-rescaled_size[1],rescaled_size[0],gt_img.size[1])\n",
    "gt_img.paste(gt_crop_scaled,pos_paste_crop)\n",
    "\n",
    "draw.rectangle(pos_paste_crop, outline='black', width=3)\n",
    "print(\"Position where scaled crop is pasted\",pos_paste_crop)\n",
    "\n",
    "# Draw the lines to connect both the boxes\n",
    "draw.line((pos_paste_crop[0],pos_paste_crop[1])+(borders_crop[0],borders_crop[1]),fill='black',width=3)\n",
    "draw.line((pos_paste_crop[2],pos_paste_crop[3])+(borders_crop[2],borders_crop[3]),fill='black',width=3)\n",
    "\n",
    "# gt_img.show()\n",
    "gt_img.save(created_img)"
   ]
  }
 ]
}