{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python36964bit455cf936f5d74eed9470ab769c9b3f8c",
   "display_name": "Python 3.6.9 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "### Loading the models trained on Ejecta isosurface representation and saving the predicted images"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from model import models\n",
    "from PIL import ImageDraw\n",
    "from data import data_utils\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global params\n",
    "width = 512\n",
    "height = 512\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# volume params\n",
    "center = torch.Tensor([0.0, 0.0, 0.0])\n",
    "radius = 1.0\n",
    "vol_params = (\"sphere\", center, radius)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load saved model\n",
    "input_ch = 518\n",
    "points_type = 'cartesian'\n",
    "viewdir_type = 'cartesian'\n",
    "out_dir = '/home/parika/WorkingDir/Thesis/Code/dvr/outputs/ejecta/iso/AO/21_views/gauss32.0_256_ssim+l1_withSigmoid/'\n",
    "writer_path = os.path.join(out_dir,'logs')\n",
    "model_path = os.path.join(out_dir,'models/best_model_74.pth')\n",
    "\n",
    "# create the model\n",
    "model = models.ConvNet(input_ch=input_ch, output_ch=1).to(device)\n",
    "\n",
    "checkpoint = torch.load(model_path, map_location=device)\n",
    "input_map = checkpoint['input_map']\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/home/parika/WorkingDir/Thesis/Data/ejecta/iso/val/'\n",
    "vis_dir = os.path.join(out_dir + 'vis/val/')\n",
    "\n",
    "with torch.no_grad():\n",
    "    for cam_file in glob.glob(data_dir+\"*.json\"):\n",
    "        fid =cam_file[cam_file.rfind('_')+1:cam_file.rfind('.')]\n",
    "        gt_file = data_dir + \"ao_\" + fid + \".bin\"\n",
    "\n",
    "        gt = np.fromfile(gt_file, dtype=np.float32)\n",
    "        gt = np.reshape(gt,(height, width))\n",
    "        gt = torch.Tensor(gt).to(device)\n",
    "\n",
    "        points, viewdirs, _ = data_utils.get_input_data(cam_file, vol_params, points_type, viewdir_type)\n",
    "        gt = gt[50:500,50:500]\n",
    "        points = points[:,50:500,50:500]\n",
    "        viewdirs = viewdirs[:,50:500,50:500]\n",
    "\n",
    "        mask = torch.ones(size=gt.shape,dtype=torch.bool,device=gt.device)\n",
    "\n",
    "        input = torch.cat((points, viewdirs),dim=0).to(device)\n",
    "        input = data_utils.input_mapping(input, input_map, True, True, \"cartesian\",\"ConvNet\")\n",
    "        input = input.unsqueeze_(0)\n",
    "        ao = model(input)[0,0]\n",
    "\n",
    "        ao = torch.clamp(ao.detach().cpu(), min=0.0, max=1.0)\n",
    "        gt_img_file = vis_dir + \"gt_\" + fid + \".png\"\n",
    "        pred_img_file = vis_dir + \"pred_\" + fid + \".png\"\n",
    "\n",
    "        gt = Image.fromarray((gt * 255.0).numpy().astype(np.uint8))\n",
    "        ao = Image.fromarray((ao * 255.0).numpy().astype(np.uint8))\n",
    "        gt.save(gt_img_file)\n",
    "        ao.save(pred_img_file)"
   ]
  },
  {
   "source": [
    "### Create the zoomed in images for AO predictions and ground truth"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_file = '/home/goel/Downloads/ssim/img1.png'\n",
    "created_img = '/home/goel/Downloads/ssim/img1_cropped.png'\n",
    "gt_img = Image.open(img_file)\n",
    "gt_img = gt_img.crop((80,80,480,480))\n",
    "gt_img.load()\n",
    "\n",
    "# borders_crop = (210,210,280,280)\n",
    "# borders_crop = (220,70,290,140)\n",
    "borders_crop = (180, 90, 250, 160)\n",
    "# borders_crop = (30,190,100,260)\n",
    "# borders_crop = (150, 90, 220, 160)\n",
    "gt_crop = gt_img.crop(borders_crop)\n",
    "print(\"Size of the crop taken: \", gt_crop.size)\n",
    "\n",
    "height, width = gt_crop.size\n",
    "rescaled_size=(height*3, width*3)\n",
    "gt_crop_scaled =  gt_crop.resize(rescaled_size)\n",
    "print(\"Size of the scaled crop: \",gt_crop_scaled.size)\n",
    "\n",
    "# Draw a box to specify where the crop is taken\n",
    "width_of_rect = 1.0\n",
    "draw = ImageDraw.Draw(gt_img)\n",
    "draw.rectangle(borders_crop, outline='black', width=3)\n",
    "print(\"Position where the crop is taken: \",borders_crop)\n",
    "\n",
    "# Draw a box where the crop will be pasted\n",
    "# pos_paste_crop = (0,0,rescaled_size[0],rescaled_size[1])\n",
    "# draw.rectangle(pos_paste_crop, outline='black', width=3)\n",
    "# print(\"Position where scaled crop is pasted\",pos_paste_crop)\n",
    "\n",
    "# pos_paste_crop = (0,0,rescaled_size[0],rescaled_size[1])\n",
    "# pos_paste_crop = (gt_img.size[0]-rescaled_size[0],gt_img.size[1]-rescaled_size[1],gt_img.size[0],gt_img.size[1])\n",
    "pos_paste_crop = (0,gt_img.size[1]-rescaled_size[1],rescaled_size[0],gt_img.size[1])\n",
    "gt_img.paste(gt_crop_scaled,pos_paste_crop)\n",
    "\n",
    "draw.rectangle(pos_paste_crop, outline='black', width=3)\n",
    "print(\"Position where scaled crop is pasted\",pos_paste_crop)\n",
    "\n",
    "# Draw the lines to connect both the boxes\n",
    "draw.line((pos_paste_crop[0],pos_paste_crop[1])+(borders_crop[0],borders_crop[1]),fill='black',width=3)\n",
    "draw.line((pos_paste_crop[2],pos_paste_crop[3])+(borders_crop[2],borders_crop[3]),fill='black',width=3)\n",
    "\n",
    "# gt_img.show()\n",
    "gt_img.save(created_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate tensorboard ao images into individual images\n",
    "img_file = '/home/goel/Downloads/l1+ssim.png'\n",
    "img = Image.open(img_file)\n",
    "\n",
    "first_img = img.crop((0,0,512,512))\n",
    "first_img.save('/home/goel/Downloads/img1.png')\n",
    "\n",
    "second_img = img.crop((513,0,1024,512))\n",
    "second_img.save('/home/goel/Downloads/img2.png')"
   ]
  }
 ]
}